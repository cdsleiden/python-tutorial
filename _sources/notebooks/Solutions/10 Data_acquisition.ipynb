{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct downloads\n",
    "\n",
    "### Exercise 10.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addresses of texts created for the [Project Gutenberg](https://www.gutenberg.org) website.\n",
    "\n",
    "```\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python that downloads all the files in this list and stores them in the current directory.\n",
    "As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt').\n",
    "The basename in a URL can be extracted using the [`os.path.basename()`](https://docs.python.org/3/library/os.path.html#os.path.basename) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "# Recreate the given list using copy and paste\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "\n",
    "# We use a for-loop to take the same steps for each item in the list:\n",
    "for url in urls:\n",
    "    # 1. Download the file contents\n",
    "    response = requests.get(url)\n",
    "    # 1a. Force the textual contents to be interpreted as UTF-8 encoded, because the website does not send the text encoding\n",
    "    response.encoding = 'utf-8'\n",
    "    # 2. Use basename to get a suitable filename\n",
    "    filename = os.path.basename(url)\n",
    "    # 3. Open the file in write mode and write the downloaded file contents to the file\n",
    "    out = open( filename , mode = 'w', encoding= 'utf-8' )\n",
    "    out.write( response.text )\n",
    "    # 4. Close the file\n",
    "    out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring data via APIs\n",
    "\n",
    "### Exercise 10.4.\n",
    "\n",
    "Find the coordinates for each address in the given list using [OpenStreetMap](https://www.openstreetmap.org/)'s Nominatim API.\n",
    "\n",
    "The Nominatim API can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is <https://nominatim.openstreetmap.org/search>.\n",
    "\n",
    "Following the `q` parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the `format` parameter, you can use `xml` for XML-formatted data or `json` for JSON-formatted data. Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```\n",
    "\n",
    "The JSON data received via the OpenStreetMap API can be converted to regular Python lists and dictionaries using the `json()` method: \n",
    "\n",
    "```json_data = response.json()```\n",
    "\n",
    "If the result is saved as variable named `json_data`, you should be able to access the latitude and the longitude as follows:\n",
    "\n",
    "```\n",
    "latitude = json_data[0]['lat']\n",
    "longitude = json_data[0]['lon']\n",
    "```\n",
    "\n",
    "The `[0]` is used to get the results for the first result.\n",
    "\n",
    "Print each address and its latitude and longitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    url = f'https://nominatim.openstreetmap.org/search?q={a}&format=json'\n",
    "\n",
    "    response = requests.get( url ) # The spaces in each address are automatically encoded as '%20' by requests\n",
    "    json_data = response.json()\n",
    "    # json_data is a list of results; we assume that the first result is always correct(!)\n",
    "    latitude = json_data[0]['lat']\n",
    "    longitude = json_data[0]['lon']\n",
    "    print( f'{latitude},{longitude}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below also creates an interactive map, with markers on the locations described in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "locations_coord = dict()\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    # You can 'URL-encode' the spaces in the addresses yourself, instead of leaving it to requests\n",
    "    a_encoded = urllib.parse.quote(a)\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a_encoded + '&format=json'\n",
    "    print(url)\n",
    "\n",
    "    response = requests.get( url )\n",
    "    json_data = response.json() \n",
    "    latitude = json_data[0]['lat']\n",
    "    longitude = json_data[0]['lon']\n",
    "    locations_coord[a] = (latitude,longitude)\n",
    "    print( f'{latitude},{longitude}')\n",
    "\n",
    "\n",
    "out = open( 'map.html' , 'w' , encoding = 'utf-8')\n",
    "\n",
    "out.write('''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\n",
    "    <title>Locations</title>\n",
    "\n",
    "    <meta charset=\"utf-8\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\n",
    "    <link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.css\" integrity=\"sha512-xodZBNTC5n17Xt2atTPuE1HxjVMSvLVW9ocqUKLsCC5CXdbqCmblAshOMAS6/keqq/sMZMZ19scR4PsZChSR7A==\" crossorigin=\"\"/>\n",
    "    <script src=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.js\" integrity=\"sha512-XQoYMqMTK8LvdxXYG3nZ448hOEQiglfqkJs1NOQV44cWnUrBc8PkAOcXy20w0vlaXaVUearIOBhiXZ5V3ynxwA==\" crossorigin=\"\"></script>\n",
    "\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div id=\"mapid\" style=\"width: 800px; height: 600px;\"></div>\n",
    "<script>\n",
    "\n",
    "    var mymap = L.map('mapid').setView([52.1568157,4.4850392], 6);\n",
    "\n",
    "    L.tileLayer('https://api.mapbox.com/styles/v1/{id}/tiles/{z}/{x}/{y}?access_token=pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw', {\n",
    "                maxZoom: 18,\n",
    "                attribution: 'Map data &copy; <a href=\"https://www.openstreetmap.org/\">OpenStreetMap</a> contributors, ' +\n",
    "                             '<a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA</a>, ' +\n",
    "                             'Imagery  <a href=\"https://www.mapbox.com/\">Mapbox</a>',\n",
    "                id: 'mapbox/streets-v11',\n",
    "                tileSize: 512,\n",
    "                zoomOffset: -1\n",
    "    }).addTo(mymap); \n",
    "''')\n",
    "\n",
    "for l in locations_coord:\n",
    "    display_name = re.sub( '\\'' , '' , l )\n",
    "    out.write( f' L.marker([ { locations_coord[l][0] }, { locations_coord[l][1] }  ]).addTo(mymap) ')\n",
    "    out.write( f\" .bindPopup('{display_name}.') \")  \n",
    "    out.write( ';' )\n",
    "    \n",
    "out.write(\n",
    "'''\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "# Do not forget to close the file handler!\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.8.\n",
    "\n",
    "As was discussed in this notebook, you can use the *MusicBrainz* API to request information about musicians. Via the code that is provided, you can request the names and the type (i.e. are we dealing with a person or with a group?). This specific API can make much more information available, however. Try to add some code with can add the following data about each artist: \n",
    "\n",
    "* The date of birth (in the case of a person) or formation (in the case of a group)\n",
    "* The date of death or breakup\n",
    "* The place of birth or formation\n",
    "* The place of death or breakup\n",
    "* Aliases\n",
    "* Tags associated with the artist\n",
    "\n",
    "Tip: 'Uncomment' the print statement in the second cell to be able explore the structure of the JSON data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.utils import requote_uri\n",
    "\n",
    "\n",
    "root_url = 'https://musicbrainz.org/ws/2/'\n",
    "\n",
    "## The parameters for the API call are defined as variables\n",
    "entity = 'artist'\n",
    "query = 'David Bowie'\n",
    "limit = 5\n",
    "fmt = 'json'\n",
    "\n",
    "query = requote_uri(query)\n",
    "\n",
    "api_call = f'{root_url}{entity}?query={query}&fmt={fmt}&limit={limit}'\n",
    "response = requests.get( api_call )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "musicbrainz_results = response.json()\n",
    "\n",
    "for artist in musicbrainz_results['artists']:\n",
    "    #print(json.dumps(artist, indent=4))\n",
    "    name = artist.get('name','[unknown]')\n",
    "    artist_type = artist.get('type','[unknown]')\n",
    "    print(f'{name} ({artist_type})')\n",
    "\n",
    "    begin = ''\n",
    "    end = ''\n",
    "    \n",
    "    if 'life-span' in artist:\n",
    "        begin = artist['life-span'].get('begin','[unknown]')\n",
    "        if artist['life-span']['ended']:\n",
    "            end = artist['life-span'].get('end','[unknown]')\n",
    "            \n",
    "    if 'begin-area' in artist:\n",
    "        begin += ' in ' + artist['begin-area'].get('sort-name','[unknown]')\n",
    "    if 'end-area' in artist:\n",
    "        end += ' in ' + artist['end-area'].get('sort-name','[unknown]')\n",
    "        \n",
    "    ## Add your code below\n",
    "    if artist_type == 'Person':\n",
    "        print('Born',end=': ')\n",
    "    else:\n",
    "        print('Formation',end=': ')\n",
    "    print(begin)\n",
    "    \n",
    "    if len(end)>0:\n",
    "        if artist_type == 'Person':\n",
    "            print('Died',end=': ')\n",
    "        else:\n",
    "            print('Breakup',end=': ')\n",
    "        print(end)\n",
    "    \n",
    "    aliases = []\n",
    "    if 'aliases' in artist:\n",
    "        for alias in artist['aliases']:\n",
    "            aliases.append(alias['sort-name'])\n",
    "        \n",
    "    if len(aliases)>0:\n",
    "        print('Aliases',end=': ')\n",
    "        print( ', '.join(aliases) )\n",
    "        \n",
    "    tags = []\n",
    "    if 'tags' in artist:\n",
    "        for tag in artist['tags']:\n",
    "            tags.append(tag['name'])\n",
    "        \n",
    "    if len(tags)>0:\n",
    "        print('Tags',end=': ')\n",
    "        print( ', '.join(tags) )\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.9.\n",
    "\n",
    "*[PLOS One](https://journals.plos.org/plosone/)* is a peer reviewed open access journal. The *PLOS One* API can be used to request metadata about all the articles that have been published in the journal. In this API, you can refer to specific articles using their [DOI](https://www.doi.org/).\n",
    "\n",
    "Such requests can be sent using API calls with the following structure:\n",
    "\n",
    "https://api.plos.org/search?q=id:{doi}\n",
    "\n",
    "To acquire data about the article with DOI [10.1371/journal.pone.0270739](https://doi.org/10.1371/journal.pone.0270739), for example, you can use the following API call:\n",
    "\n",
    "https://api.plos.org/search?q=id:10.1371/journal.pone.0270739\n",
    "\n",
    "Try to write code which can get hold of metadata about the articles with the following DOIs:\n",
    "\n",
    "* 10.1371/journal.pone.0169045\n",
    "* 10.1371/journal.pone.0271074\n",
    "* 10.1371/journal.pone.0268993\n",
    "\n",
    "For each article, print the title, the publication date, the article type, a list of all the authors and the abstract. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dois = [ '10.1371/journal.pone.0169045','10.1371/journal.pone.0268993','10.1371/journal.pone.0271074' ]\n",
    "\n",
    "\n",
    "root_url = 'https://api.plos.org/search?q=id:'\n",
    "\n",
    "## The parameters for the API call are defined as variables\n",
    "for doi in dois:\n",
    "\n",
    "    api_call = f'{root_url}{doi}'\n",
    "    print(api_call)\n",
    "\n",
    "    response = requests.get( api_call )\n",
    "\n",
    "    if response: \n",
    "        plos_results = response.json()\n",
    "        for article in plos_results['response']['docs']:\n",
    "            #print(article)\n",
    "            print(article['title_display'])\n",
    "            print(article['article_type'])\n",
    "            print(article['publication_date'])\n",
    "            authors = article['author_display']\n",
    "            for author in authors:\n",
    "                print(author)\n",
    "            print(article['abstract'][0].strip())\n",
    "            print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "### Exercise 10.5.\n",
    "\n",
    "The webpage below offers access to the complete work of the author H.P. Lovecraft. \n",
    "\n",
    "<https://www.hplovecraft.com/writings/texts/>\n",
    "    \n",
    "Write code in Python to find and print the URLs of all the texts that are listed. The links are all encoded in an element named &lt;a&gt;. The attribute `href` mentions the links, and the body of the &lt;a&gt; element mentions the title. List only the web pages that end in '.aspx'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "base_url = \"http://www.hplovecraft.com/writings/texts/\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "if response: \n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup( response.text ,\"lxml\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        if link.get('href') is not None:\n",
    "            title = link.string\n",
    "            url = base_url + link.get('href')\n",
    "            if re.search( r'aspx$' , url): \n",
    "                print( f'{title}\\n{url}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.6.\n",
    "\n",
    "Using `requests` and `BeautifulSoup`, create a list of all the countries mentioned on https://www.scrapethissite.com/pages/simple/.\n",
    "\n",
    "Also collect and print data about the capital, the population and the area of all of these countries.\n",
    "\n",
    "How you print or present the information is not too important here; the challenge in this exercise is to extract the data from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text\n",
    "    \n",
    "    \n",
    "soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    \n",
    "countries = soup.find_all('div', {'class': 'col-md-4 country'} )\n",
    "\n",
    "\n",
    "for c in countries:\n",
    "    \n",
    "    name = c.find('h3' , { 'class':'country-name'})\n",
    "    print(name.text.strip())\n",
    "    \n",
    "    capital = c.find('span', { 'class':'country-capital'}).text\n",
    "    population = c.find('span', { 'class':'country-population'}).text\n",
    "    area = c.find('span', { 'class':'country-area'}).text\n",
    "    \n",
    "    print(f'  Capital: {capital}')\n",
    "    print(f'  Population: {population}')\n",
    "    print(f'  Area: {area}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.7.\n",
    "\n",
    "Download all the images shown on the following page: <https://www.bbc.com/news/in-pictures-61014501>\n",
    "\n",
    "You can follow these steps:\n",
    "\n",
    "1. Download the HTML file\n",
    "1. 'Scrape' the HTML file you downloaded. As images in HTML are encoded using the `<img>` element, try to create a list containing all occurrences of this element. \n",
    "1. Find the URLS of all the images. Within these `<img>` element, there should be a `src` attribute containing the URL of the image. \n",
    "1. The bbc.com website uses images as part of the user interface. These images all have the word 'line' in their filenames. Try to exclude these images whose file names contain the word 'line'. \n",
    "1. Download all the images that you found in this way, using the `requests` library. In the `Response` object that is created following a succesful download, you need to work with the `content` property to obtain the actual file.  Save all these images on your computer, using `open()` and `write()`. In the `open()` function, use the string `\"wb\"` (write binary) as a second parameter (instead of only `\"w\"`) to make sure that the contents are saved as bytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = 'https://www.bbc.com/news/in-pictures-61014501'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response:\n",
    "    html_page = response.text\n",
    "    soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    images = soup.find_all('img')\n",
    "    for i in images:\n",
    "        img_url = i.get('src')\n",
    "        if 'line' not in img_url:\n",
    "            response = requests.get(img_url)\n",
    "            if response:\n",
    "                file_name = os.path.basename(img_url)\n",
    "                print(file_name)\n",
    "                out = open( file_name , 'wb' )\n",
    "                out.write(response.content)\n",
    "                out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional challenges\n",
    "\n",
    "### Exercise 10.2.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's construct the full API call (which is a URL) piece by piece\n",
    "base_url = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "search_term = \"Dutch\"\n",
    "limit = 30\n",
    "data_format = 'json'\n",
    "\n",
    "api_call = f'{base_url}&search={search_term}&limit={limit}&format={data_format}'\n",
    "\n",
    "# Get the data using the Requests library\n",
    "response_data = requests.get( api_call )\n",
    "\n",
    "# Because we asked for and got JSON-formatted data, Requests lets us access\n",
    "# the data as a Python data structure using the .json() method\n",
    "wiki_results = response_data.json()\n",
    "\n",
    "# Now we print the search results \n",
    "for i in range( 0 , len(wiki_results[1]) ):\n",
    "    print( 'Title: ' + wiki_results[1][i] )\n",
    "    print( 'Tagline: ' + wiki_results[2][i] )\n",
    "    print( 'Url: ' + wiki_results[3][i] + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3.\n",
    "\n",
    "Write an application in Python that extracts all the publications that have been added to a specific ORCID account, using the ORCID API.\n",
    "\n",
    "Information about individual ORCID accounts can be obtained by appending their ID to the base URL <https://pub.orcid.org/v2.0/>. The ORCID API returns data in XML by default. In the XML, the list of publications can be found using the XPath `r:record/a:activities-summary/a:works/a:group` (using the namespace declarations given below).\n",
    "\n",
    "*Note: we use the [ElementTree](https://docs.python.org/3/library/xml.etree.elementtree.html) library to process the XML data. It is very powerful, but has a quite steep learning curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orcid = '0000-0002-8469-6804'\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Declare namespace abbreviations\n",
    "ns = {'o': 'http://www.orcid.org/ns/orcid' ,\n",
    "'s' : 'http://www.orcid.org/ns/search' ,\n",
    "'h': 'http://www.orcid.org/ns/history' ,\n",
    "'p': 'http://www.orcid.org/ns/person' ,\n",
    "'pd': 'http://www.orcid.org/ns/personal-details' ,\n",
    "'a': 'http://www.orcid.org/ns/activities' ,\n",
    "'e': 'http://www.orcid.org/ns/employment' ,\n",
    "'c': 'http://www.orcid.org/ns/common' , \n",
    "'w': 'http://www.orcid.org/ns/work',\n",
    "'r': 'http://www.orcid.org/ns/record'}\n",
    "\n",
    "\n",
    "try:\n",
    "    # Construct the API call and retrieve the data\n",
    "    orcidUrl = \"https://pub.orcid.org/v2.0/\" + orcid\n",
    "    print( orcidUrl )\n",
    "    \n",
    "    response = requests.get( orcidUrl )\n",
    "    \n",
    "    # Parse XML string into its Python ElementTree object representation\n",
    "    root = ET.fromstring(response.text)\n",
    "    \n",
    "    # Find and print the ORCID creation date\n",
    "    creationDate = root.find('h:history/h:submission-date' , ns ).text\n",
    "    \n",
    "    print('\\nORCID created on:')\n",
    "    print(creationDate)\n",
    "    \n",
    "    # Print the title and DOI of each work (DOI only when available)\n",
    "    print('\\nWorks:')\n",
    "    \n",
    "    works = root.findall('a:activities-summary/a:works/a:group' , ns )\n",
    "    for w in works:\n",
    "        title = w.find('w:work-summary/w:title/c:title' , ns ).text\n",
    "        print(title)\n",
    "        doiEl = w.find('c:external-ids/c:external-id/c:external-id-url' , ns )\n",
    "        if doiEl is not None:\n",
    "            doi = doiEl.text\n",
    "            print(doi)\n",
    "            \n",
    "except:\n",
    "    print(\"Data could not be downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
