{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addresses of texts created for the [Project Gutenberg](https://www.gutenberg.org) website.\n",
    "\n",
    "```\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python that downloads all the files in this list and stores them in the current directory.\n",
    "As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt').\n",
    "The basename in a URL can be extracted using the [`os.path.basename()`](https://docs.python.org/3/library/os.path.html#os.path.basename) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "\n",
    "# Recreate the given list using copy and paste\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "\n",
    "# We use a for-loop to take the same steps for each item in the list:\n",
    "for url in urls:\n",
    "    # 1. Download the file contents\n",
    "    response = requests.get(url)\n",
    "    # 1a. Force the textual contents to be interpreted as UTF-8 encoded, because the website does not send the text encoding\n",
    "    response.encoding = 'utf-8'\n",
    "    # 2. Use basename to get a suitable filename\n",
    "    filename = os.path.basename(url)\n",
    "    # 3. Open the file in write mode and write the downloaded file contents to the file\n",
    "    out = open( filename , mode = 'w', encoding= 'utf-8' )\n",
    "    out.write( response.text )\n",
    "    # 4. Close the file\n",
    "    out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.2.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only.\n",
    "\n",
    "*Hint: the tutorial covers the Wikipedia API.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's construct the full API call (which is a URL) piece by piece\n",
    "baseURL = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "searchTerm = \"Dutch\"\n",
    "limit = 30\n",
    "data_format = 'json'\n",
    "\n",
    "apiCall = '{}&search={}&limit={}&format={}'.format( baseURL, searchTerm , limit , data_format )\n",
    "\n",
    "# Get the data using the Requests library\n",
    "responseData = requests.get( apiCall )\n",
    "\n",
    "# Because we asked for and got JSON-formatted data, Requests lets us access\n",
    "# the data as a Python data structure using the .json() method\n",
    "wikiResults = responseData.json()\n",
    "\n",
    "# Now we print the search results \n",
    "for i in range( 0 , len(wikiResults[1]) ):\n",
    "    print( 'Title: ' + wikiResults[1][i] )\n",
    "    print( 'Tagline: ' + wikiResults[2][i] )\n",
    "    print( 'Url: ' + wikiResults[3][i] + '\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.3.\n",
    "\n",
    "Write an application in Python that extracts all the publications that have been added to a specific ORCID account, using the ORCID API.\n",
    "\n",
    "Information about individual ORCID accounts can be obtained by appending their ID to the base URL <https://pub.orcid.org/v2.0/>. The ORCID API returns data in XML by default. In the XML, the list of publications can be found using the XPath `r:record/a:activities-summary/a:works/a:group` (using the namespace declarations given below).\n",
    "\n",
    "*Note: we use the [ElementTree](https://docs.python.org/3/library/xml.etree.elementtree.html) library to process the XML data. It is very powerful, but has a quite steep learning curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orcid = '0000-0002-8469-6804'\n",
    "\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Declare namespace abbreviations\n",
    "ns = {'o': 'http://www.orcid.org/ns/orcid' ,\n",
    "'s' : 'http://www.orcid.org/ns/search' ,\n",
    "'h': 'http://www.orcid.org/ns/history' ,\n",
    "'p': 'http://www.orcid.org/ns/person' ,\n",
    "'pd': 'http://www.orcid.org/ns/personal-details' ,\n",
    "'a': 'http://www.orcid.org/ns/activities' ,\n",
    "'e': 'http://www.orcid.org/ns/employment' ,\n",
    "'c': 'http://www.orcid.org/ns/common' , \n",
    "'w': 'http://www.orcid.org/ns/work',\n",
    "'r': 'http://www.orcid.org/ns/record'}\n",
    "\n",
    "\n",
    "try:\n",
    "    # Construct the API call and retrieve the data\n",
    "    orcidUrl = \"https://pub.orcid.org/v2.0/\" + orcid\n",
    "    print( orcidUrl )\n",
    "    \n",
    "    response = requests.get( orcidUrl )\n",
    "    \n",
    "    # Parse XML string into its Python ElementTree object representation\n",
    "    root = ET.fromstring(response.text)\n",
    "    \n",
    "    # Find and print the ORCID creation date\n",
    "    creationDate = root.find('h:history/h:submission-date' , ns ).text\n",
    "    \n",
    "    print('\\nORCID created on:')\n",
    "    print(creationDate)\n",
    "    \n",
    "    # Print the title and DOI of each work (DOI only when available)\n",
    "    print('\\nWorks:')\n",
    "    \n",
    "    works = root.findall('a:activities-summary/a:works/a:group' , ns )\n",
    "    for w in works:\n",
    "        title = w.find('w:work-summary/w:title/c:title' , ns ).text\n",
    "        print(title)\n",
    "        doiEl = w.find('c:external-ids/c:external-id/c:external-id-url' , ns )\n",
    "        if doiEl is not None:\n",
    "            doi = doiEl.text\n",
    "            print(doi)\n",
    "            \n",
    "except:\n",
    "    print(\"Data could not be downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.4.\n",
    "\n",
    "The API developed by [OpenStreetMap](https://www.openstreetmap.org/) can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is https://nominatim.openstreetmap.org/search. Following the `q` parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the `format` parameter, you can use `xml` for XML-formatted data or `json` for JSON-formatted data. Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "import re\n",
    "import string\n",
    "from os.path import isfile, join , isdir\n",
    "import os\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a_encoded + '&format=json'\n",
    "\n",
    "    response = requests.get( url )\n",
    "    json_data = response.json() \n",
    "    latitude = json_data[0]['lat']\n",
    "    longitude = json_data[0]['lon']\n",
    "    print( f'{latitude},{longitude}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below also creates an interactive map, with markers on the locations described in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import urllib.parse\n",
    "import re\n",
    "import string\n",
    "from os.path import isfile, join , isdir\n",
    "import os\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "locations_coord = dict()\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    a_encoded = urllib.parse.quote(a)\n",
    "    url = 'https://nominatim.openstreetmap.org/search?q='+ a_encoded + '&format=json'\n",
    "    print(url)\n",
    "\n",
    "    response = requests.get( url )\n",
    "    json_data = response.json() \n",
    "    latitude = json_data[0]['lat']\n",
    "    longitude = json_data[0]['lon']\n",
    "    locations_coord[a] = (latitude,longitude)\n",
    "    print( f'{latitude},{longitude}')\n",
    "\n",
    "\n",
    "out = open( 'map.html' , 'w' , encoding = 'utf-8')\n",
    "import re\n",
    "\n",
    "out.write('''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\n",
    "                <title>Locations</title>\n",
    "\n",
    "                <meta charset=\"utf-8\" />\n",
    "                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\n",
    "                <link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"docs/images/favicon.ico\" />\n",
    "\n",
    "    <link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.css\" integrity=\"sha512-xodZBNTC5n17Xt2atTPuE1HxjVMSvLVW9ocqUKLsCC5CXdbqCmblAshOMAS6/keqq/sMZMZ19scR4PsZChSR7A==\" crossorigin=\"\"/>\n",
    "    <script src=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.js\" integrity=\"sha512-XQoYMqMTK8LvdxXYG3nZ448hOEQiglfqkJs1NOQV44cWnUrBc8PkAOcXy20w0vlaXaVUearIOBhiXZ5V3ynxwA==\" crossorigin=\"\"></script>\n",
    "\n",
    "\n",
    "\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\n",
    "\n",
    "<div id=\"mapid\" style=\"width: 800px; height: 600px;\"></div>\n",
    "<script>\n",
    "\n",
    "                var mymap = L.map('mapid').setView([52.1568157,4.4850392], 6);\n",
    "\n",
    "                L.tileLayer('https://api.mapbox.com/styles/v1/{id}/tiles/{z}/{x}/{y}?access_token=pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw', {\n",
    "                                maxZoom: 18,\n",
    "                                attribution: 'Map data &copy; <a href=\"https://www.openstreetmap.org/\">OpenStreetMap</a> contributors, ' +\n",
    "                                                '<a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA</a>, ' +\n",
    "                                                'Imagery  <a href=\"https://www.mapbox.com/\">Mapbox</a>',\n",
    "                                id: 'mapbox/streets-v11',\n",
    "                                tileSize: 512,\n",
    "                                zoomOffset: -1\n",
    "                }).addTo(mymap); \n",
    "''')\n",
    "\n",
    "for l in locations_coord:\n",
    "    display_name = re.sub( '\\'' , '' , l )\n",
    "    out.write( f' L.marker([ { locations_coord[l][0] }, { locations_coord[l][1] }  ]).addTo(mymap) ')\n",
    "    out.write( f\" .bindPopup('{display_name}.') \")  \n",
    "    out.write( ';' )\n",
    "    \n",
    "out.write(\n",
    "'''\n",
    "</script>\n",
    "\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "''')\n",
    "\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.5\n",
    "\n",
    "The webpage below offers access to the complete work of the author H.P. Lovecraft. \n",
    "\n",
    "http://www.hplovecraft.com/writings/texts/\n",
    "\n",
    "    \n",
    "Write code in Python to find the URLs of all the texts that are listed. The links are all encoded in an element named &lt;a&gt;. The attribute `href` mentions the links, and the body of the &lt;a&gt; element mentions the title. List only the web pages that end in '.aspx'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "base_url = \"http://www.hplovecraft.com/writings/texts/\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "if response: \n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup( response.text ,\"lxml\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        if link.get('href') is not None:\n",
    "            title = link.string\n",
    "            url = base_url + link.get('href')\n",
    "            if re.search( r'aspx$' , url): \n",
    "                print( f'{title}\\n{url}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.6\n",
    "\n",
    "Using `requests` and `BeautifulSoup`, create a list of all the countries mentioned on https://www.scrapethissite.com/pages/simple/.\n",
    "\n",
    "Also collect data about the capital, the population and the area of all of these countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text\n",
    "    \n",
    "    \n",
    "soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    \n",
    "countries = soup.find_all('div', {'class': 'col-md-4 country'} )\n",
    "\n",
    "\n",
    "for c in countries:\n",
    "    \n",
    "    name = c.find('h3' , { 'class':'country-name'})\n",
    "    print(name.text.strip())\n",
    "    \n",
    "    # find all <span> elements underneath <h3 class='country_name'>\n",
    "    span = c.find_all(\"span\" )\n",
    "    \n",
    "    capital = ''\n",
    "    population = 0\n",
    "    area = 0\n",
    "    \n",
    "    for s in span:\n",
    "\n",
    "        if s['class'][0] == 'country-capital':\n",
    "            capital = s.text\n",
    "        \n",
    "        if s['class'][0] == 'country-population':\n",
    "            population = s.text\n",
    "            \n",
    "        if s['class'][0] == 'country-area':\n",
    "            area = s.text\n",
    "    \n",
    "    print(f'  Capital: {capital}')\n",
    "    print(f'  Population: {population}')\n",
    "    print(f'  Area: {area}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.7\n",
    "\n",
    "Download all the images shown on the following page: https://www.bbc.com/news/in-pictures-61014501 \n",
    "\n",
    "You can follow these steps:\n",
    "\n",
    "* Download the HTML file\n",
    "* 'Scrape' the HTML file you downloaded. As images in HTML are encoded using the `img` element, try to create a list containing all occurrences of this element. \n",
    "* Find the URLS of all the images. Witnin these `img` element, there should be a `src` attribute containing the URL of the image. \n",
    "* The bbc.com website uses images as part of the user interface. These images all have the word 'line' in their filenames. Try to exclude these images whose file names contain the word 'line'. \n",
    "* Download all the images that you found in this way, using the `requests` library. In the `Response` object that is created following a succesful download, you need to work with the `content` property to obtain the actual file.  Save all these images on your computer, using `open()` and `write()`. In the `open()` function, use the code ‘wb’ as a second parameter (instead of only ‘w’) to make sure that the contents are saved as bytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = 'https://www.bbc.com/news/in-pictures-61014501'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response:\n",
    "    html_page = response.text\n",
    "    soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    images = soup.find_all('img')\n",
    "    for i in images:\n",
    "        img_url = i.get('src')\n",
    "        if 'line' not in img_url:\n",
    "            response = requests.get(img_url)\n",
    "            if response:\n",
    "                file_name = os.path.basename(img_url)\n",
    "                print(file_name)\n",
    "                out = open( file_name , 'wb' )\n",
    "                out.write(response.content)\n",
    "                out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
