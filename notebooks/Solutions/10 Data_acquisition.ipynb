{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data acquisition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.1.\n",
    "\n",
    "The list below contains a number of URLs. They are the web addresses of texts created for the [Project Gutenberg](https://www.gutenberg.org) website.\n",
    "\n",
    "```\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "```\n",
    "\n",
    "Write a program in Python that can download all the files in this list and stores them in the current directory.\n",
    "\n",
    "As filenames, use the same names that are used by Project Gutenberg (e.g. '580-0.txt' or '1400-0.txt').\n",
    "\n",
    "The basename in a URL can be extracted using the [`os.path.basename()`](https://docs.python.org/3/library/os.path.html#os.path.basename) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path\n",
    "\n",
    "# Recreate the given list using copy and paste\n",
    "urls = [ 'https://www.gutenberg.org/files/580/580-0.txt' ,\n",
    "'https://www.gutenberg.org/files/1400/1400-0.txt' ,\n",
    "'https://www.gutenberg.org/files/786/786-0.txt' ,\n",
    "'https://www.gutenberg.org/files/766/766-0.txt' \n",
    "]\n",
    "\n",
    "# We use a for-loop to take the same steps for each item in the list:\n",
    "for url in urls:\n",
    "    # 1. Download the file contents\n",
    "    response = requests.get(url)\n",
    "    # 1a. Force the textual contents to be interpreted as UTF-8 encoded, because the website does not send the text encoding\n",
    "    response.encoding = 'utf-8'\n",
    "    # 2. Use basename to get a suitable filename\n",
    "    filename = os.path.basename(url)\n",
    "    # 3. Open the file in write mode and write the downloaded file contents to the file\n",
    "    out = open( filename , mode = 'w', encoding= 'utf-8' )\n",
    "    out.write( response.text )\n",
    "    # 4. Close the file\n",
    "    out.close()\n",
    "    \n",
    "print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2.\n",
    "\n",
    "As was discussed, you can use the *MusicBrainz* API to request information about musicians. Via the code that is provided, you can request the names and the types of artists. This specific API can make much more information available, however. Try to add some code with can add the following data about each artist: \n",
    "\n",
    "* The gender (in the case of a person)\n",
    "* The date of birth (in the case of a person) or formation (in the case of a group)\n",
    "* Aliases\n",
    "\n",
    "If you want to see the structure of the JSON data, you can 'uncomment' the print statement in the second cell to be able explore the structure of the JSON data. \n",
    "\n",
    "The information about the date of birth or the date of formation is available via the key `life-span`. The value associated with this key is yet another dictionary. This second dictionary has the keys you need, namely `start` and `end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.utils import requote_uri\n",
    "\n",
    "\n",
    "root_url = 'https://musicbrainz.org/ws/2/'\n",
    "\n",
    "## The parameters for the API call are defined as variables\n",
    "entity = 'artist'\n",
    "query = 'David Bowie'\n",
    "limit = 5\n",
    "fmt = 'json'\n",
    "\n",
    "query = requote_uri(query)\n",
    "\n",
    "api_call = f'{root_url}{entity}?query={query}&fmt={fmt}&limit={limit}'\n",
    "response = requests.get( api_call )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "musicbrainz_results = response.json()\n",
    "\n",
    "for artist in musicbrainz_results['artists']:\n",
    "    #print(json.dumps(artist, indent=4))\n",
    "    name = artist.get('name','[unknown]')\n",
    "    artist_type = artist.get('type','[unknown]')\n",
    "    print(f'{name} ({artist_type})')\n",
    "    \n",
    "    \n",
    "    if artist_type == 'Person':\n",
    "        if 'gender' in artist:\n",
    "            print(f'Gender: {artist[\"gender\"].title()}')\n",
    "\n",
    "    begin = ''\n",
    "    \n",
    "    if 'life-span' in artist:\n",
    "        begin = artist['life-span'].get('begin','[unknown]')\n",
    "\n",
    "        if artist_type == 'Person':\n",
    "            print('Born',end=': ')\n",
    "        else:\n",
    "            print('Formation',end=': ')\n",
    "        print(begin)\n",
    "    \n",
    "    \n",
    "    aliases = []\n",
    "    if 'aliases' in artist:\n",
    "        for alias in artist['aliases']:\n",
    "            aliases.append(alias['sort-name'])\n",
    "        \n",
    "    if len(aliases)>0:\n",
    "        print('Aliases',end=': ')\n",
    "        print( ', '.join(aliases) )\n",
    "        \n",
    "    print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3.\n",
    "\n",
    "Find the coordinates for each address in the given list using [OpenStreetMap](https://www.openstreetmap.org/)'s Nominatim API.\n",
    "\n",
    "The Nominatim API can be used, among other things, to find the precise geographic coordinates of a specific location. The base URL of this API is <https://nominatim.openstreetmap.org/search>.\n",
    "\n",
    "Following the `q` parameter, you need to supply a string describing the locations whose latitude and longitude you want to find. As values for the `format` parameter, you can use `xml` for XML-formatted data or `json` for JSON-formatted data. \n",
    "\n",
    "Use this API to find the longitude and the latitude of the addresses in the following list:\n",
    "\n",
    "```\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "```\n",
    "\n",
    "The JSON data received via the OpenStreetMap API can be converted to regular Python lists and dictionaries using the `json()` method: \n",
    "\n",
    "```json_data = response.json()```\n",
    "\n",
    "If the result is saved as variable named `json_data`, you should be able to access the latitude and the longitude as follows:\n",
    "\n",
    "```\n",
    "latitude = json_data[0]['lat']\n",
    "longitude = json_data[0]['lon']\n",
    "```\n",
    "\n",
    "The `[0]` is used to get the results for the first result.\n",
    "\n",
    "Print each address and its latitude and longitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "addresses = ['Grote Looiersstraat 17 Maastricht' , 'Witte Singel 27 Leiden' ,\n",
    "'Singel 425 Amsterdam' , 'Drift 27 Utrecht' , 'Broerstraat 4 Groningen']\n",
    "\n",
    "\n",
    "for a in addresses:\n",
    "    url = f'https://nominatim.openstreetmap.org/search?q={a}&format=json'\n",
    "\n",
    "    response = requests.get( url ) # The spaces in each address are automatically encoded as '%20' by requests\n",
    "    json_data = response.json()\n",
    "    # json_data is a list of results; we assume that the first result is always correct(!)\n",
    "    latitude = json_data[0]['lat']\n",
    "    longitude = json_data[0]['lon']\n",
    "    print( f'{latitude},{longitude}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.4.\n",
    "\n",
    "*[PLOS One](https://journals.plos.org/plosone/)* is a peer reviewed open access journal. The *PLOS One* API can be used to request metadata about all the articles that have been published in the journal. In this API, you can refer to specific articles using their [DOI](https://www.doi.org/).\n",
    "\n",
    "Such requests can be sent using API calls with the following structure:\n",
    "\n",
    "https://api.plos.org/search?q=id:{doi}\n",
    "\n",
    "To acquire data about the article with DOI [10.1371/journal.pone.0270739](https://doi.org/10.1371/journal.pone.0270739), for example, you can use the following API call:\n",
    "\n",
    "https://api.plos.org/search?q=id:10.1371/journal.pone.0270739\n",
    "\n",
    "Try to write code which can get hold of metadata about the articles with the following DOIs:\n",
    "\n",
    "* 10.1371/journal.pone.0169045\n",
    "* 10.1371/journal.pone.0271074\n",
    "* 10.1371/journal.pone.0268993\n",
    "\n",
    "For each article, print the title, the publication date, the article type, a list of all the authors and the abstract. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "dois = [ '10.1371/journal.pone.0169045','10.1371/journal.pone.0268993','10.1371/journal.pone.0271074' ]\n",
    "\n",
    "\n",
    "root_url = 'https://api.plos.org/search?q=id:'\n",
    "\n",
    "## The parameters for the API call are defined as variables\n",
    "for doi in dois:\n",
    "\n",
    "    api_call = f'{root_url}{doi}'\n",
    "    print(api_call)\n",
    "\n",
    "    response = requests.get( api_call )\n",
    "\n",
    "    if response: \n",
    "        plos_results = response.json()\n",
    "        for article in plos_results['response']['docs']:\n",
    "            #print(article)\n",
    "            print(article['title_display'])\n",
    "            print(article['article_type'])\n",
    "            print(article['publication_date'])\n",
    "            authors = article['author_display']\n",
    "            for author in authors:\n",
    "                print(author)\n",
    "            print(article['abstract'][0].strip())\n",
    "            print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.5.\n",
    "\n",
    "This tutorial has explained how you can extract data about the titles and the prices of all the books that are shown on the web page [http://books.toscrape.com/](http://books.toscrape.com/). \n",
    "\n",
    "Can you write code to extract the URLs of all the book covers on this page? These URLs can be found in the `src` attribute of the `<img>` elements within the `<article>` about each book. Note that the `<img>` element specifies a relative path. To change the relative path into an absolute path, you need to concatenate the base url ([http://books.toscrape.com/](http://books.toscrape.com/) and the relative path to the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://books.toscrape.com/'\n",
    "response = requests.get( url )\n",
    "\n",
    "\n",
    "if response:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text \n",
    "    \n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "all_books = soup.find_all( 'article' , {'class':'product_pod'} )\n",
    "    \n",
    "for book in all_books:\n",
    "    i = book.find('img')\n",
    "    image_url = url + i.get('src')\n",
    "    print(image_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.6. \n",
    "\n",
    "On the web page [http://books.toscrape.com/](http://books.toscrape.com/), the menu on the lefthand side contains a list of all the subject categories of the books. \n",
    "\n",
    "Try to write some code which can extract all the terms in this list. This list is in an element named `div`, and this `<div>` has a `class` attribute with the value `side_categories`. The categories themselves are all encoded within an `<a>` element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'http://books.toscrape.com/'\n",
    "response = requests.get( url )\n",
    "\n",
    "\n",
    "if response:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text \n",
    "    \n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "div = soup.find( 'div' , {'class':'side_categories'} )\n",
    "\n",
    "all_categories = div.find_all('a')\n",
    "    \n",
    "for category in all_categories:\n",
    "    print(category.text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.7.\n",
    "\n",
    "Write a program in Python which can extract data from the following web page:\n",
    "\n",
    "[https://www.imdb.com/chart/top/](https://www.imdb.com/chart/top/)\n",
    "\n",
    "This is a page on the [Internet Movie Database website](https://www.imdb.com). It lists the 25 most highly rated movies.\n",
    "\n",
    "More specifically, try to extract the titles of these movies and the URLs of the pages on IMDB. \n",
    "\n",
    "If you inspect the source code of this web page, you can see that the information about the movies is encoded as follows:\n",
    "\n",
    "```\n",
    "<td class=\"titleColumn\">\n",
    "\n",
    "<a href=\"/title/tt0068646/\">\n",
    "The Godfather\n",
    "</a>\n",
    "\n",
    "</td>\n",
    "```\n",
    "\n",
    "The data can found in a `<td>` element whose `class` attribute has value `titleColumn`. `td` stands for 'table data'. This HTML element is used to create a cell in a table. The actual title in given in a hyperlink, encoded using `<a>`. The URL to the page for the movie is given in an 'href' attribute. \n",
    "\n",
    "There is one additional challenge that you need to be aware of. The IMDB website only responds to requests received from web scraping scripts if these requests also specify a 'User-Agent' in the a header. Each HTTP request contains a header, which provides important metadata about the request. The 'User-Agent' in this header typically gives imformation about the Computer and the browser from which the request was sent. \n",
    "\n",
    "The easiest way to find an approproate value for a 'User-Agent' is to go to [a website listing some common options](https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-headers/), and to select the case that applies. \n",
    "\n",
    "The information about the 'User-Agent' in the header must be provided in the `header` attribute of the `get()` method of `requests`, in the form of a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.imdb.com/chart/top/'\n",
    "\n",
    "## You can use the value below if you use Firefox on a Mac\n",
    "## Adjust the value of user_agent if that is not the case.\n",
    "user_agent = '\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:98.0) Gecko/20100101 Firefox/98.0\"'\n",
    "headers={\"User-Agent\": user_agent}\n",
    "\n",
    "response = requests.get( url , headers=headers)\n",
    "\n",
    "print(response.status_code)\n",
    "\n",
    "if response:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = soup.find_all('td', {'class': 'titleColumn'} )\n",
    "\n",
    "for m in movies:\n",
    "    # Find links (a elements) within the cell\",\n",
    "    children = m.findChildren(\"a\" , recursive=False)\n",
    "    for c in children:\n",
    "        movie_title = c.text\n",
    "        url = c.get('href')\n",
    "        ## This is an internal link, so we need to prepend the base url\n",
    "        url = 'https://imdb.com' + url\n",
    "        print( f'{movie_title}: {url}' )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.8.\n",
    "\n",
    "The webpage below offers access to the complete work of the author H.P. Lovecraft. \n",
    "\n",
    "https://www.hplovecraft.com/writings/texts/\n",
    "\n",
    "    \n",
    "Write code in Python to find and print the URLs of all the texts that are listed. The links are all encoded in an element named &lt;a&gt;. The attribute `href` mentions the links, and the body of the &lt;a&gt; element mentions the title. List only the web pages that end in '.aspx'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "base_url = \"http://www.hplovecraft.com/writings/texts/\"\n",
    "\n",
    "response = requests.get(base_url)\n",
    "if response: \n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup( response.text ,\"lxml\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        if link.get('href') is not None:\n",
    "            title = link.string\n",
    "            url = base_url + link.get('href')\n",
    "            if re.search( r'aspx$' , url): \n",
    "                print( f'{title}\\n{url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.9.\n",
    "\n",
    "Using `requests` and `BeautifulSoup`, create a list of all the countries mentioned on https://www.scrapethissite.com/pages/simple/.\n",
    "\n",
    "Also collect and print data about the capital, the population and the area of all of these countries.\n",
    "\n",
    "How you print or present the information is not too important here; the challenge in this exercise is to extract the data from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    html_page = response.text\n",
    "    \n",
    "    \n",
    "soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    \n",
    "countries = soup.find_all('div', {'class': 'col-md-4 country'} )\n",
    "\n",
    "\n",
    "for c in countries:\n",
    "    \n",
    "    name = c.find('h3' , { 'class':'country-name'})\n",
    "    print(name.text.strip())\n",
    "    \n",
    "    capital = c.find('span', { 'class':'country-capital'}).text\n",
    "    population = c.find('span', { 'class':'country-population'}).text\n",
    "    area = c.find('span', { 'class':'country-area'}).text\n",
    "    \n",
    "    print(f'  Capital: {capital}')\n",
    "    print(f'  Population: {population}')\n",
    "    print(f'  Area: {area}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.10.\n",
    "\n",
    "Download all the images shown on the following page: <https://www.bbc.com/news/in-pictures-61014501>\n",
    "\n",
    "You can follow these steps:\n",
    "\n",
    "1. Download the HTML file\n",
    "1. 'Scrape' the HTML file you downloaded. As images in HTML are encoded using the `<img>` element, try to create a list containing all occurrences of this element. \n",
    "1. Find the URLS of all the images. Within these `<img>` element, there should be a `src` attribute containing the URL of the image. \n",
    "1. The bbc.com website uses images as part of the user interface. These images all have the word 'line' in their filenames. Try to exclude these images whose file names contain the word 'line'. \n",
    "1. Download all the images that you found in this way, using the `requests` library. In the `Response` object that is created following a succesful download, you need to work with the `content` property to obtain the actual file.  Save all these images on your computer, using `open()` and `write()`. In the `open()` function, use the string `\"wb\"` (write binary) as a second parameter (instead of only `\"w\"`) to make sure that the contents are saved as bytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "url = 'https://www.bbc.com/news/in-pictures-61014501'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response:\n",
    "    html_page = response.text\n",
    "    soup = BeautifulSoup( html_page,\"lxml\")\n",
    "    images = soup.find_all('img')\n",
    "    for i in images:\n",
    "        img_url = i.get('src')\n",
    "        if 'line' not in img_url:\n",
    "            response = requests.get(img_url)\n",
    "            if response:\n",
    "                file_name = os.path.basename(img_url)\n",
    "                print(file_name)\n",
    "                out = open( file_name , 'wb' )\n",
    "                out.write(response.content)\n",
    "                out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.11.\n",
    "\n",
    "Write Python code which can download the titles and the URLs of Wikipedia articles whose titles contain the word 'Dutch'. Your code needs to display the first 30 results only.\n",
    "\n",
    "You can search for Wikipedia pages containing a certain term using the following base URL: \n",
    "\n",
    "```\n",
    "base_url = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "```\n",
    "\n",
    "\n",
    "As you can read in the [documentation of this API](https://www.mediawiki.org/w/api.php?action=help&modules=opensearch), the `opensearch` function accepts the following parameters: \n",
    "\n",
    "* *query* speficies the search term. \n",
    "* *limit* sets a limit to the number of items to return\n",
    "* For the *format*, you can choose either 'xml' or 'json'. \n",
    "\n",
    "If you request data in the JSON format, and convert the data using the `json()` method of `requests`, these data will be structured in quite a particular way. At the first level, the object is list containing four items. The second item is another list, containing the titles of the articles. The fourth item is yet another list, containing the URLs of all of these articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Let's construct the full API call (which is a URL) piece by piece\n",
    "base_url = 'https://en.wikipedia.org/w/api.php?action=opensearch'\n",
    "\n",
    "search_term = \"Dutch\"\n",
    "limit = 30\n",
    "data_format = 'json'\n",
    "\n",
    "api_call = f'{base_url}&search={search_term}&limit={limit}&format={data_format}'\n",
    "\n",
    "# Get the data using the Requests library\n",
    "response_data = requests.get( api_call )\n",
    "\n",
    "# Because we asked for and got JSON-formatted data, Requests lets us access\n",
    "# the data as a Python data structure using the .json() method\n",
    "wiki_results = response_data.json()\n",
    "\n",
    "# Now we print the search results \n",
    "for i in range( 0 , len(wiki_results[1]) ):\n",
    "    print( 'Title: ' + wiki_results[1][i] )\n",
    "    print( 'Tagline: ' + wiki_results[2][i] )\n",
    "    print( 'Url: ' + wiki_results[3][i] + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
